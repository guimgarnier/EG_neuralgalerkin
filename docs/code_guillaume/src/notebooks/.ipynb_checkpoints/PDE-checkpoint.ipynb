{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension problem\n",
    "**1) Space  domain $\\mathcal{X} \\subset \\mathbb{R}^d, d\\in \\mathbb{N}, d\\neq 0$.**\n",
    "\n",
    "**2) Time domain $ t \\in [0, \\infty)$**. \n",
    "Let us put some precisions: Let us consider the PDE\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\partial_t u(t,x)=f(t,x,u)\\quad \\textrm{for} \\;\\; (t,x)\\in [0,\\infty)\\times {\\cal X}\\\\\n",
    "u(0,x)=u_0(x)\\quad \\;\\; x \\in {\\cal X}\n",
    "\\end{cases}$$\n",
    "where $u \\in {\\cal U}$ a function space. \n",
    "\n",
    "$$ f: [0,\\infty)\\times {\\cal X}\\times {\\cal U}\\longrightarrow {\\mathbb R}\n",
    "$$\n",
    "and $f(t,x,u)=b(t,x) \\cdot\\nabla u+a(t,x):\\nabla\\nabla u+ G(t,x,u)$ for some $b:[0,\\infty)\\times {\\cal X} \\longrightarrow {\\mathbb R}^d, a: [0,\\infty)\\times {\\cal X} \\longrightarrow {\\mathbb R}^d\\times {\\mathbb R}^d$ and $G: [0,\\infty)\\times {\\cal X}\\times {\\mathbb R} \\longrightarrow {\\mathbb R}$.\n",
    "\n",
    "\n",
    " We assume that $\\mathcal{X} = \\Pi_{j=1}^d[a_j,b_j]$ and a sampling in $j$ axis has $n_j$ points. Then the discrete $\\mathcal{X}$  will be like **torch.tensor()** of dimension $n_1\\times n_2\\times n_3\\times\\dots\\times n_d$. The PDE classes are created such that one must input a 2D sampling with $n$ rows and $d$ columns like\n",
    "$$\n",
    "X=\\begin{pmatrix}\n",
    "x^1\\\\\n",
    "x^2\\\\\n",
    "\\vdots\\\\\n",
    "x^n\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "where $x^i=(x_1^i,x_2^i,\\dots,x_d^i), t \\in \\mathbb{R}^+$. The output will be \n",
    "$$\n",
    "f(t,X,u)=\\begin{pmatrix}\n",
    "f(t,x^1,u)\\\\\n",
    "f(t,x^2,u)\\\\\n",
    "\\vdots\\\\\n",
    "f(t,x^n,u)\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "sys.path.append(module_path)\n",
    "\n",
    "import torch\n",
    "from lib.PDE import KdV,AllenCahn,AD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text case for KDV Eq\n",
    "\n",
    "KdV eq: $$\n",
    "\\partial_t u=-\\partial^3_xu+6u\\partial_xu$$\n",
    "\n",
    "In this case test, we asssume that $u_0(x)=\\sin(x)$ and $u(t,x)=t.x^3-2x^2+3$.\n",
    "We set $t \\in (0,1)$ and $x \\in [-2,4]$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 7.0175,  4.6047, 16.5640, 15.2800, -9.4791,  4.9664, -0.8811,  9.7568,\n",
      "         8.1684, -6.5737], grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "def u0(x):\n",
    "    return torch.sin(x)\n",
    "\n",
    "KDV=KdV(u0)\n",
    "\n",
    "def u(t,x):\n",
    "    return t*x**3-2*x**2+3\n",
    "    #return torch.dot(x,x)*t+2\n",
    "\n",
    "def f_function(t,x,u):\n",
    "        if t==0:\n",
    "             return u0(x)\n",
    "        else:\n",
    "            return -6*t+6*u(t,x)*(3*t*x**2-4*x)\n",
    "            #return 5.e-2*2*t*torch.ones(len(x))-(1.05+t*torch.sin(x))*(u(t,x)-u(t,x)**3)\n",
    "\n",
    "time_point=torch.linspace(0,1,10)\n",
    "t=time_point[3]\n",
    "x=torch.linspace(-2,4,10)\n",
    "x=torch.randn(10,3)\n",
    "print(KDV.f(t,x,u))\n",
    "#print(f_function(t,x,u))\n",
    "#print(KDV.f(t,x,u)==f_function(t,x,u))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test case for Allen-Cahn eq:\n",
    "$$\n",
    "\\partial_t u=\\epsilon\\partial_x^2u -a(t,x)(u-u^3)\\quad\n",
    " u_0(x)=\\sin(x)\n",
    "$$\n",
    "where \n",
    "$\\epsilon=5\\times 10^{-2}, a(t,x)=1.05+t\\sin(x)$, $t\\in (0,1)$ and $x \\in [0,2\\pi)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True, True, True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"\n",
    "Domain case\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def u0(x):\n",
    "    return torch.sin(x)\n",
    "\n",
    "AC=AllenCahn(u0)\n",
    "\n",
    "def u(t,x):\n",
    "    return t*x**2\n",
    "\n",
    "epsilon=5e-2\n",
    "def f_acfunction(t,x,u):\n",
    "        if t==0:\n",
    "             return u0(x)\n",
    "        else:\n",
    "            return 2*epsilon*t*torch.ones(len(x))-(1.05+t*torch.sin(x))*(u(t,x)-u(t,x)**3)\n",
    "time_point=torch.linspace(0,1,10)\n",
    "print(AC.f(t,x,u)==f_acfunction(t,x,u))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advection Difusion problem \n",
    "$$\n",
    "    \\partial_tu=-a(t,x)\\partial_xu$$\n",
    "1) **The case where $a$ depends only on $t$**\n",
    "    $$\n",
    "        a(t,x)=a_s\\odot(\\sin(a_v\\pi t)+5/4)\n",
    "    $$\n",
    "    with $a_s=[1,2,\\dots,d]^T, a_v=2+\\frac{2}{d}[0,1,\\dots,d-1]^T$ and  $\\odot$ the element-wise vector multiplication .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -2.5119,  -8.9750,   5.4853,   5.1429,  -5.6051, -12.4155,   7.6041,\n",
       "          1.4173,  11.1957,  -1.6495,   3.8927,   1.2920,  11.6952,  -6.1626,\n",
       "         -1.1277,  -3.5652,  -4.9422,  -6.7087,   2.6422,  -2.6851],\n",
       "       grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def u0(x):\n",
    "    return torch.sin(x)\n",
    "\n",
    "Ad_space_time=AD(u0,\"time_only\")\n",
    "\n",
    "def u(t,x):\n",
    "    #return t*torch.dot(x,x)#x*t+2\n",
    "    return x*t+2\n",
    "\n",
    "\n",
    "def funct_a(t,x):\n",
    "    x1=x[0].clone()\n",
    "    if x1.shape==torch.Size([]):\n",
    "        a_s=1\n",
    "        a_v=2\n",
    "    else:\n",
    "        a_s=torch.arange(1,len(x)+1)\n",
    "        a_v=2+2*torch.arange(0,len(x))\n",
    "    #return a_s*(torch.sin(a_v*torch.pi*t)+3)*(x+1)/10\n",
    "    return a_s*(torch.sin(a_v*torch.pi*t)+5/4)\n",
    "\n",
    "def ad_function_space_time(t,x,funct_a):\n",
    "    return t*funct_a(t,x)\n",
    "\n",
    "\n",
    "time_point=torch.linspace(0,1,10)\n",
    "t=time_point[2]\n",
    "x=torch.linspace(0,2*torch.pi,10)\n",
    "x1=x[0:2]\n",
    "\n",
    "x=torch.randn(20,4)\n",
    "#Ad_space_time.f(t,x,u)\n",
    "print(ad_function_space_time(t,x,funct_a)==Ad_space_time.f(t,x,u))\n",
    "#print(x1.shape==torch.Size([]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advection Difusion problem \n",
    "$$\n",
    "    \\partial_tu=-a(t,x)\\partial_xu$$\n",
    "2) **The case where $a$ depends on $t$ and $x$**\n",
    "    $$\n",
    "        a(t,x)=a_s\\odot(\\sin(a_v\\pi t)+5/4)\\odot(x+1)/10\n",
    "    $$\n",
    "    with $a_s=[1,2,\\dots,d]^T, a_v=2+\\frac{2}{d}[0,1,\\dots,d-1]^T$ and  $\\odot$ the element-wise vector multiplication .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0886, 0.1504, 0.2122, 0.2740, 0.3358, 0.3977, 0.4595, 0.5213, 0.5831,\n",
       "        0.6449], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def u0(x):\n",
    "    return torch.sin(x)\n",
    "\n",
    "Ad_space_time=AD(u0,\"space_time\")\n",
    "\n",
    "def u(t,x):\n",
    "    return t*x+1\n",
    "    #return torch.dot(x,x)+3\n",
    "\n",
    "\n",
    "def funct_a(t,x):\n",
    "    x1=x[0].clone()\n",
    "    if x1.shape==torch.Size([]):\n",
    "        a_s=1\n",
    "        a_v=2\n",
    "    else:\n",
    "        a_s=torch.arange(1,len(x)+1)\n",
    "        a_v=2+2*torch.arange(0,len(x))\n",
    "    return a_s*(torch.sin(a_v*torch.pi*t)+3)*(x+1)/10\n",
    "    #return a_s*(torch.sin(a_v*torch.pi*t)+5/4)\n",
    "\n",
    "def ad_function_space_time(t,x,funct_a):\n",
    "    return t*funct_a(t,x)\n",
    "\n",
    "\n",
    "time_point=torch.linspace(0,1,10)\n",
    "t=time_point[2]\n",
    "x=torch.linspace(0,2*torch.pi,10)\n",
    "x1=x[0:2]\n",
    "#x=torch.randn(10,4)\n",
    "\n",
    "Ad_space_time.f(t,x,u)\n",
    "#print(ad_function_space_time(t,x,funct_a)==Ad_space_time.f(t,x,u))\n",
    "#print(x1.shape==torch.Size([]))\n",
    "#torch.tensor(x.shape).shape\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
