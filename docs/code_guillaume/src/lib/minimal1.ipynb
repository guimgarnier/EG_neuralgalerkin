{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' def f(x, y):\\n  return x + y ** 2\\nx, y = torch.randn(5), torch.randn(5)\\njacobian = jacfwd(lambda x,y: x+y**2, argnums=1)(x, y)\\nexpected = torch.diag(2 * y)\\nassert torch.allclose(jacobian, expected)\\njacobian\\n '"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\"\"\" module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "sys.path.append(module_path) \"\"\"\n",
    "import warnings\n",
    "import torch\n",
    "import torch as tc\n",
    "from torch.func import jacfwd, vmap, grad,jacrev\n",
    "\n",
    "from Model import Model\n",
    "from matrixM import MatrixM\n",
    "from vecF import VecF\n",
    "from PDE import KdV\n",
    "\n",
    "\n",
    "\"\"\" def f(x, y):\n",
    "  return x + y ** 2\n",
    "x, y = torch.randn(5), torch.randn(5)\n",
    "jacobian = jacfwd(lambda x,y: x+y**2, argnums=1)(x, y)\n",
    "expected = torch.diag(2 * y)\n",
    "assert torch.allclose(jacobian, expected)\n",
    "jacobian\n",
    " \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Model(tc.nn.Module):\n",
    "    \n",
    "    def __init__(self, d, m, theta=None, kind='gaussian'):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.m = m       # model parameter \n",
    "        self.d = d       # dimension of b (same as x)\n",
    "        \n",
    "        # size of theta according to the the model\n",
    "        if kind == 'gaussian':\n",
    "            self.s = 2*m+d*m \n",
    "        elif kind== 'nn':\n",
    "            self.L = 0.5\n",
    "            self.l = 3\n",
    "            self.s = m + m*d * self.l + d * self.l \n",
    "        else:\n",
    "            self.s = 2*m+d*m\n",
    "            warnings.warn(\"The type of network has not The theta parameter has not been initialized\") \n",
    "        \n",
    "        # Dictionary of models \n",
    "        methods = {\n",
    "            'gaussian'        : self.gaussian_x,   # Eq. (16)\n",
    "        }\n",
    "\n",
    "        # Warning : une initialisation à ones fait échouer la minimisation pour theta0 !\n",
    "        self.Theta = tc.rand(self.s,requires_grad=True) #if theta is None else theta\n",
    "        self.theta = tc.nn.Parameter(tc.rand(self.s) if theta is None else theta)\n",
    "         #self.theta = tc.nn.Parameter(2.0*tc.ones(self.s) if theta is None else theta) \n",
    "\n",
    "        # batched derivatives , input are (d,)\n",
    "        self.eval_x    = methods[kind]\n",
    "        self.forward   = vmap(self.eval_x)\n",
    "        self.dx        = vmap(grad(self.eval_x),             in_dims=(0,) )\n",
    "        self.dxx       = vmap(grad(grad(self.eval_x)),       in_dims=(0,) )\n",
    "        self.dxxx      = vmap(grad(grad(grad(self.eval_x))), in_dims=(0,) )\n",
    "\n",
    "        # work in progress\n",
    "        # self.dtheta = vmap(lambda x : autograd(self.eval_x(x), self.theta)[0])\n",
    "\n",
    "        \n",
    "    def splitter_gaussian(self):\n",
    "        c = self.theta[:self.m]\n",
    "        b = self.theta[self.m: self.m+self.m*self.d ].reshape(self.m,self.d)\n",
    "        w = self.theta[self.m+self.m*self.d:]\n",
    "        return (c,b,w)\n",
    "    def splitter_gaussian1(self):\n",
    "        c = self.Theta[:self.m]\n",
    "        b = self.Theta[self.m: self.m+self.m*self.d ].reshape(self.m,self.d)\n",
    "        w = self.Theta[self.m+self.m*self.d:]\n",
    "        return (c,b,w)\n",
    "    \n",
    "    def splitter_nn(self):\n",
    "        c = self.theta[:self.m]\n",
    "        b=self.theta[self.m:self.m+self.d*self.l].reshape(self.l,self.d)\n",
    "        w=self.theta[self.m+self.d*self.l:].reshape(self.l,self.m,self.d)\n",
    "        return (c,b,w)\n",
    "    \n",
    "    def gaussian_x(self, x: tc.Tensor):\n",
    "               \n",
    "       # Input :  a SCALAR x of shape (0,)\n",
    "       # Output : a SCALAR\n",
    "        \n",
    "        c, b, w = self.splitter_gaussian()\n",
    "        return tc.sum(c*tc.exp(-w**2 *(x-b).T**2))  \n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def forward_t(self, theta, x):\n",
    "        self.theta = theta\n",
    "        c,b,w = self.splitter_gaussian()\n",
    "        diff = x.unsqueeze(2)-b.t().unsqueeze(0) # tensor with shape (npoints,d,m)\n",
    "        distance = torch.norm(diff, dim=1)**2 # tensor of dimension (npoints, m)\n",
    "        exp_factor = torch.exp(-w.unsqueeze(0)**2*distance)\n",
    "        return torch.sum(c.unsqueeze(0)*exp_factor, dim=1).unsqueeze(1) \"\"\"\n",
    "    def forward_t(self, theta, x):\n",
    "        self.Theta = theta.clone()\n",
    "        c,b,w = self.splitter_gaussian1()\n",
    "        #vect=lambda x: (c*tc.exp(-w**2*(x-b)**2)).sum()\n",
    "        #return (c*tc.exp(-w**2*(x-b)**2)).sum()\n",
    "        return (c*tc.exp(-w**2*(x-b)**2)).sum()\n",
    "    \n",
    "    def grad_theta(self, theta, samples):\n",
    "        self.Theta = theta.clone()\n",
    "        jacobian=jacfwd(self.forward_t,argnums=0)(self.Theta, samples)\n",
    "        return jacobian\n",
    "        \n",
    "    def out_prod(self,theta,x):\n",
    "        #self.Theta=theta.clone()\n",
    "        out_p=lambda theta,x_: tc.outer(self.grad_theta(theta,x_),self.grad_theta(theta,x_))\n",
    "        M=vmap((out_p),in_dims=(None,0))(theta,x)\n",
    "        return tc.mean(M,dim=0)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3169, 0.8238, 0.5502, 0.3625, 0.5025, 0.7829], requires_grad=True)\n",
      "tensor([[ 3.8204e+00,  3.7001e+00, -3.9899e-01, -2.7961e-02, -1.0932e-01,\n",
      "         -4.1885e-01],\n",
      "        [ 3.7001e+00,  3.5848e+00, -3.8494e-01, -2.5010e-02, -1.0463e-01,\n",
      "         -4.0098e-01],\n",
      "        [-3.9899e-01, -3.8494e-01,  1.2599e-01,  8.7895e-02,  1.2755e-02,\n",
      "          4.8916e-02],\n",
      "        [-2.7961e-02, -2.5010e-02,  8.7895e-02,  8.6085e-02,  2.6807e-03,\n",
      "          1.0266e-02],\n",
      "        [-1.0932e-01, -1.0463e-01,  1.2755e-02,  2.6807e-03,  4.2777e-03,\n",
      "          1.6279e-02],\n",
      "        [-4.1885e-01, -4.0098e-01,  4.8916e-02,  1.0266e-02,  1.6279e-02,\n",
      "          6.1964e-02]], grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "d = 1\n",
    "m = 2\n",
    "s=2*m+d*m \n",
    "theta=tc.rand(s,requires_grad=True)\n",
    "print(theta)\n",
    "NN = Model(d,m,theta)\n",
    "samples = torch.rand(10, d)\n",
    "result_grad = NN.grad_theta(theta, samples[1])\n",
    "result_out = NN.out_prod(theta, samples)\n",
    "print(result_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3169, 0.8238, 0.5502, 0.3625, 0.5025, 0.7829], requires_grad=True)\n",
      "tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "M1=MatrixM(d,m,samples)\n",
    "print(M1(theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0.], grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "pde = KdV(T = 4, xmin=-10, xmax=20, numberTimes=300)\n",
    "t=1\n",
    "VF=VecF(pde.d,NN.m,pde.f,samples)\n",
    "print(VF(t,theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3169, 0.8238, 0.5502, 0.3625, 0.5025, 0.7829], requires_grad=True)\n",
      "tensor([0.3169, 0.8238, 0.5502, 0.3625, 0.5025, 0.7829], requires_grad=True)\n",
      "tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "tensor([0., 0., 0., 0., 0., 0.], grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(theta)\n",
    "MM=NN.out_prod(theta,samples)\n",
    "#print(MM)\n",
    "M_1=MatrixM(d,m,samples)\n",
    "MM1=M_1(theta)\n",
    "print(MM1)\n",
    "#print(MM1)\n",
    "g=tc.allclose(MM,MM1)\n",
    "#print(g) \n",
    "pde = KdV(T = 4, xmin=-10, xmax=20, numberTimes=300)\n",
    "#Fapprox = VecF(self.pde.d, self.model.m, self.pde.f, samples, kind = self.kind)\n",
    "VF=VecF(pde.d,NN.m,pde.f,samples)\n",
    "t=0.1\n",
    "#NN(sample)\n",
    "print(VF(t,theta))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' tensorproduct = torch.zeros(samples.size()[0], NN.theta.size()[0], NN.theta.size()[0])\\nfor iter in range(0, samples.size()[0]):\\n    tensorproduct[iter] = torch.outer(result_grad[iter], result_grad[iter]) \\n    \\ntorch.mean(tensorproduct, dim=0).shape '"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" tensorproduct = torch.zeros(samples.size()[0], NN.theta.size()[0], NN.theta.size()[0])\n",
    "for iter in range(0, samples.size()[0]):\n",
    "    tensorproduct[iter] = torch.outer(result_grad[iter], result_grad[iter]) \n",
    "    \n",
    "torch.mean(tensorproduct, dim=0).shape \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' def u(theta,x):\\n    return x**2*theta.dot(theta)\\nn=10\\nspace_x=tc.linspace(0,4,100)\\ntheta=tc.rand(n)\\ndef grad_u(theta,x):\\n    return jacrev(u,argnums=0)(theta,x)\\ndef out_prod(theta,vect_x):\\n    M=lambda vect,x: tc.outer(grad_u(vect,x),grad_u(vect,x))\\n    return vmap((M),in_dims=(None,0))(theta,vect_x)\\n    \\nM=tc.mean(out_prod(theta,space_x),dim=0)\\nprint(M.shape) '"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" def u(theta,x):\n",
    "    return x**2*theta.dot(theta)\n",
    "n=10\n",
    "space_x=tc.linspace(0,4,100)\n",
    "theta=tc.rand(n)\n",
    "def grad_u(theta,x):\n",
    "    return jacrev(u,argnums=0)(theta,x)\n",
    "def out_prod(theta,vect_x):\n",
    "    M=lambda vect,x: tc.outer(grad_u(vect,x),grad_u(vect,x))\n",
    "    return vmap((M),in_dims=(None,0))(theta,vect_x)\n",
    "    \n",
    "M=tc.mean(out_prod(theta,space_x),dim=0)\n",
    "print(M.shape) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "vect_x=tc.rand(2,requires_grad=True)\n",
    "vect_y=vect_x.clone()\n",
    "print(vect_y.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inverse-neural-galerkin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
